== Resiliency

In this section we'll discuss resiliency in the world of microservices.
First we'll have a look at what solutions Istio offers us already, transparent to the application, and afterwards we'll see what can be implemented with MicroProfile.

In order to not get confused by different behavior of coffee-shop `v1` and `v2`, let's first revert the changes of the last section, and only have a single subset `v1`.
This means, we change the coffee-shop virtual service, destination rule to the original one (`kubectl apply`), and remove the deployment `coffee-shop-v2` (`kubectl delete deployment coffee-shop-v2`).
After the deployment has been deleted, the pods with version 2 will also be terminated.

NOTE: Although we can always change infrastructure-as-code files and apply them via `kubectl apply`, deletions of resources in files won't be detected by Kubernetes, since it only re-applies and updates the currently provided resources.
Thus we need to explicitly `kubectl delete` obsolete resources.


=== Istio

Istio ships with some resiliency features, such as timeouts, circuit breakers, or retries.
Since the sidecar proxy containers intercept all connection from and to the application containers, they have full control over HTTP traffic.

That means, if our applications do not include for example connection timeouts, Istio can, as a last resort, ensure that a timeout will be triggered on a slow connection.
This enables us to add at least certain resiliency patterns without changing applications.

=== Timeouts

Istio enables us to add simple timeout definitions on rules.
For this to work, we need to ensure that the traffic is actually routed through virtual service rules, and not just uses the default rules, that is, the instances' labels need to match the specific subsets.

We'll enhance our coffee-shop virtual service rules with a timeout of 1 second:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: coffee-shop
spec:
  hosts:
  - "*"
  gateways:
  - coffee-shop-gateway
  http:
  - route:
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v1
    timeout: 1s
---
----

NOTE: The `timeout` definition is contained in a specific route YAML object and will only be taken into account on that very rule.

We update the virtual service to our cluster.
Now, the overall time of the coffee order requests will not exceed one second.
If the coffee-shop application, or any backend with which we communicate synchronously takes longer than that the sidecar proxy will respond with an HTTP error instead.

==== Testing resiliency

The challenge, however, is now to see whether our changes took effect as desired.
We'd expect our applications to respond in much less than one second, therefore we would not see that error situation until it's in production.
Luckily, Istio ships with functionality that purposely produces error situations, in order to test the resiliency of our services.

The sidecars have two main means to do that: adding artificial delays, and failing requests.
We can instruct the routing rules to add these fault scenarios, if required only on a given percentage of the requests.

We modify the barista virtual service to add a 3 seconds delay for 50% of the requests:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: barista
spec:
  hosts:
  - barista
  http:
  - route:
    - destination:
        host: barista
        subset: v1
    fault:
      delay:
        fixedDelay: 3s
        percent: 50
---
----

If we apply this updated resource to the cluster, we will notice that some of the connections will in fact fail, after roughly 1 second.
We don't see any request taking the whole 3 seconds, due to the timeout on the coffee-shop routing rules:

----
while true; do
curl <ip-address>:<node-port>/coffee-shop/resources/orders -i -XPOST \
  -H 'Content-Type: application/json' \
  -d '{"type":"Espresso"}' \
  | grep HTTP
sleep 1
done
----

NOTE: The `fault` property is only meant for testing purposes. Please don't apply this to any other environment where you don't want connections to be slowed down or to randomly fail.

Besides the obvious responses, we can also use our observability tools to inspect what is happening.
Have a look at the Grafana dashboards and the Jaeger traces again, to see how the failing requests are made visible.

This lab only covers timeouts and basic faults.
Istio also offers functionality for retries and circuit breakers which are also applied and configured declaratively via Istio resources.
Have a look at the further resources to learn more.


=== Application level

Building a resilient microservice is key when designing microservices.
Apart from the infrastructure resilience, sometimes more fine-grained application level resilience is required.

https://github.com/eclipse/microprofile-fault-tolerance/[MicroProfile Fault Tolerance^] provides a simple yet flexible solution to build a resilient microservice at the application-level.
It offers capabilities for timeouts, retries, bulkheads, circuit breakers and fallback, as we will see below.

==== Timeout

We can use the `@Timeout` annotation to configure timeouts and their period on methods.

[source, java]
----
@Timeout(400) // timeout is 400ms
public void callService() {
    // ...
}
----

==== Retries

The `@Retry` annotation specifies that methods will be retried on failure.

[source, java]
----
@Retry(maxRetries = 90, maxDuration= 1000)
public void callService() {
   // ...
}
----

The configured number of retries in that example is 90 and the overall maximum duration is 1000 ms.
That is, once the maximum duration is reached, no more retries should be performed, even when the number of execution has not reached the maximum number of retries.

==== Bulkheads

MicroProfile Fault Tolerance offers two types of the bulkhead patterns: thread isolation and semaphore isolation.
The `@Bulkhead` annotation is used to limit the number of concurrent executions.
See the code snippets for the difference in isolation by threads or semaphores:

[source, java]
----
// thread isolation, max 5 concurrent requests allowed, max 8 requests in the waiting queue
@Asynchronous
@Bulkhead(value = 5, waitingTaskQueue = 8)
public Future<Connection> serviceA() {
    // ...
    return CompletableFuture.completedFuture(conn);
}

// semaphore isolation
@Bulkhead(5) // maximum 5 concurrent requests allowed
public Connection serviceA() {
    // ...
    return conn;
}
----

==== Circuit Breakers

The `@CircuitBreaker` annotation defines circuit breaker behavior on our methods.
Once the execution of a method fails too many times, subsequent executions will be prevented for a certain period of time.

[source, java]
----
@CircuitBreaker(successThreshold = 10, requestVolumeThreshold = 4,
    failureRatio = 0.75, delay = 1000)
public Connection serviceA() {
    // ...
}
----

==== Fallback

Many provided functionality cannot guarantee a 100% success rate.
Thus, in many cases we need to provide backups in case executions fail for some reason.
This is, of course, dependent on the actual business uses case, which fallback behavior should be executed.

In the following example, the `emptyOrders()` method will be invoked if `getOrders()` returns exceptionally.

This is now what we will add to our coffee shop application.
We enhance the `OrdersResource` class as follows:

[source, java]
----
@GET
@Fallback(fallbackMethod = "emptyOrders")
@Retry
public JsonArray getOrders() {
    return coffeeShop.getOrders().stream()
        .map(this::buildOrder)
        .collect(JsonCollectors.toJsonArray());
}

// ...

public JsonArray emptyOrders() {
    return Json.createArrayBuilder().build();
}
----

This will output an empty list of orders in case of an error, assuming that this is what we want in our example application.

This is one resiliency example which can only be achieved on an application-level, since we define how our application reacts to the exceptional situation.
In general, MicroProfile Fault Tolerance's resiliency is more fine-grained while Istio's behavior is more coarse-grained.
As a recommendation, we can use MicroProfile together with Istio's fault handling.
For more details on MicroProfile, Istio, and their relationship, read Emily Jiang's https://www.eclipse.org/community/eclipse_newsletter/2018/september/MicroProfile_istio.php[blog post^].
