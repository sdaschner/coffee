== Docker

In order to run our applications in Docker containers, we need Docker images first which are built from ``Dockerfile``s.
Therefore, we create two `Dockerfile` files, one under each project directory.


=== Docker images

Let's start with the coffee-shop application.
Under `coffee-shop/`, we create a `Dockerfile` with the following contents:

[source,Dockerfile]
----
FROM sdaschner/open-liberty:javaee8-tracing-b1

COPY liberty/server.xml $CONFIG_DIR

COPY target/coffee-shop.war $DEPLOYMENT_DIR
----

The image declaration after `FROM` specifies the base image for our application.
We use that as a starting ground.
For now, we can assume that this base image already includes an application server, here Open Liberty, and a Java runtime.

`$CONFIG_DIR` and `$DEPLOYMENT_DIR` are Docker variables which can be used during build and run time.
This facilitates to add files to directories inside the container, whose location we don't have to manually specify.

The `COPY` command copies resources from the host to the container, that is, they will end up in the resulting image.
The `liberty/server.xml` file contains the application-specific server configuration including the server features that we want to use in our service.
Since we deploy only a single applications to each running server and thus running container, the server configuration is exclusively used for the individual applications.

Take a moment to have a brief look at the `liberty/server.xml` file.
We'll come back to it later.
Now, we create a similar `Dockerfile` for the barista application, under `barista/`:

[source,Dockerfile]
----
FROM sdaschner/open-liberty:javaee8-tracing-b1

COPY liberty/server.xml $CONFIG_DIR

COPY target/barista.war $DEPLOYMENT_DIR
----


=== Building

In order to build the images, we issue the following Docker command: `docker build -t <image>:<tag> .` (including the dot at the end).

For the coffee-shop application this looks as follows:

----
cd coffee-shop/
docker build -t registry.eu-gb.bluemix.net/think-<your-name>-workshop/coffee-shop:1 .
----

The image name, here `registry.eu-gb.bluemix.net/think-<your-name>-workshop/coffee-shop:1`, implicitly contains the location of the image registry, image name, and version (or tag).
If an image with that name is being pulled or pushed, downloaded or uploaded, respectively, Docker thus already knows the destination.

The `registry.eu-gb.bluemix.net/...` part indicates the used cloud service and our user name and namespace.

Again, we repeat this step for the barista application:

----
cd ../barista/
docker build -t registry.eu-gb.bluemix.net/think-<your-name>-workshop/barista:1 .
----

Congratulations, now we have two images stored locally, for the coffee-shop and barista applications that we can now run as Docker containers!


=== Running

If we want, we can already run the created Docker images locally on our machines.

In order do to that, we have to keep in mind, that the coffee-shop application will connect to the barista backend, thus needs to resolve and access the second running container.
If we have a look at the source code of the coffee-shop application, specifically the `Barista` gateway, we notice that the application will simply connect to `http://barista:9080` -- host name `barista`.

This will be possible since we specify that our microservices will run in a cloud-native environment, where we can control the environment independent of the applications.
Thus we make sure that our applications will be reachable under logical names, such as `coffee-shop` or `barista`.

For locally running container, we can assign names to the running containers that are resolvable within other containers that run inside the same local Docker network.
This requires to specify both the container names and the network at runtime.
We start our containers as follows:

----
docker run --rm -d \
  --name barista \
  --network dkrnet \
  registry.eu-gb.bluemix.net/think-<your-name>-workshop/barista:1

docker run --rm -d \
  --name coffee-shop \
  --network dkrnet \
  -p 9080:9080 \
  registry.eu-gb.bluemix.net/think-<your-name>-workshop/coffee-shop:1
----

These commands run our two applications as Docker containers.
The `--rm` flag will remove the containers after they have been stopped, `-d` runs the containers in the background, `--name` specifies the logical container names, `--network` makes sure both containers will run in the correct network, and the `-p` declarations forward the local containers ports to the localhost.

The two running containers can now resolve and access each other by their logical container names, which allows our coffee-shop application to connect to the barista backend using the `barista` host name.
The reason why we're doing this, is because Kubernetes will support a very similar resolution of logical names.

==== Accessing our applications

Now we can access and test our running microservices for the first time!
Since they communicate using HTTP we can use any REST client of our choice, for example `curl`.


==== Health check resources

We could simply ask the application for the coffee orders in the system but for our cloud-native environment we might want a more basic way to check whether our application is up and running.
Therefore, we create health check resources.
We could do so by using plain REST resources, for example implemented by JAX-RS.
What's also possible, and done with minimal effort, is to use MicroProfile Health to create HTTP health checks.

Therefore, we create a class with the name `HealthResource` or similar, in both the barista and coffee shop projects, for example under the `*.boundary` package:

[source,java]
----
import org.eclipse.microprofile.health.*;
import javax.enterprise.context.ApplicationScoped;

@Health
@ApplicationScoped
public class HealthResource implements HealthCheck {

    @Override
    public HealthCheckResponse call() {
        return HealthCheckResponse.named("barista").withData("barista", "ok").up().build();
    }
}
----

This shows the example for the barista application.
For our coffee shop example please adjust the strings accordingly.

That's it!
However, if we want these changes to be included in our running containers, we need to re-build the Java applications and the Docker images again, of course.

After the applications have been started, we should be able to access the coffee shop via the local port `9080` and the default MicroProfile health resource:

----
curl localhost:9080/health -i
----

This accesses the health check resource and will hopefully give you a successful HTTP response.
The `-i` flag causes the HTTP response headers to be printed.


==== Ordering coffee

Now, we can finally ask for the coffee orders:

----
curl localhost:9080/coffee-shop/resources/orders
----

This will give us the coffee orders that are in the system returned as JSON.
No orders have been created, thus the array is empty.

Let's change this and create a coffee order!

If we have a look at the JAX-RS resource in the coffee-shop application, we can see that to create a new coffee order, we have to POST a JSON object containing the coffee _type_.
Using `curl` this looks as follows:

----
curl localhost:9080/coffee-shop/resources/orders -i -XPOST \
  -H 'Content-Type: application/json' \
  -d '{"type":"Espresso"}'
----

`-XPOST` specifies the `POST` HTTP method, `-H` the HTTP header, so the service knows that we're sending the JSON content type, and `-d` specifies the data that we send as HTTP request body.
Sending this command hopefully yields us a successful `201 Created` response, the information that our coffee order is in the system.

We can double-check this by querying the resource for all coffee orders again, similar to before, which now should respond with a JSON array that contains our order.

If that's the case, congratulations!
You've just built, run, and manually tested cloud-native microservices running in Docker containers.


=== Pushing

In order to make our Docker images not just locally accessible, we will push them to a container registry in the cloud.
Then we can later pull them from any environment, like a managed Kubernetes cluster.

We push our Docker images with the following commands:

----
docker push registry.eu-gb.bluemix.net/think-<your-name>-workshop/coffee-shop:1
docker push registry.eu-gb.bluemix.net/think-<your-name>-workshop/barista:1
----

You will notice, that the second `push` commands runs much faster and outputs that almost all layers already exist in the remote repository.
This thanks to the copy-on-write file system which Docker uses internally and save us developers an enormous amount of time and bandwidth.
The same is true for re-building images.
Docker recognizes which commands of the Docker build need to be re-executed, and only performs these and the following.

This is the reason why especially for cloud-native applications it makes sense to craft thin deployment artifacts.
The WAR files that comprise our applications only contain the business logic that is part of our application, no implementation details.
The base image, i.e. the application server or its configuration doesn't change that frequently, therefore we're mostly shipping our (small) application only.

Now, that our microservices are running as Docker containers already, let's see how we bring Kubernetes into the game in the link:03-kubernetes.adoc[next section].
